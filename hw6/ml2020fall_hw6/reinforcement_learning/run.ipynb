{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. Please check the pdf file for more details.*\n",
    "\n",
    "In this exercise you will:\n",
    "\n",
    "- implement Q-learning on a few different control tasks from the OpenAI Gym benchmark suite\n",
    "- implement approximators for (potentially) infinite and continous MDP, including linear approximator and neural networks\n",
    "- explore some popular tricks in RL, including exploration/exploitability trade-off, experience replay.\n",
    "\n",
    "Please note that **YOU CANNOT USE ANY MACHINE LEARNING PACKAGE SUCH AS SKLEARN** for any homework, unless you are asked to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some basic imports\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']= 'True'\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "try:\n",
    "    import gym\n",
    "except:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install gym --user\n",
    "    import gym\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's first get to know the gym benchmark\n",
    "\n",
    "The [Gym](http://gym.openai.com/) is a famous benchmark suite from OpenAI. It provides some handful environments to play with the agents/algorithms you design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " %%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first play with a simple environment which is discrete and finite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeze Lake\n",
    "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "\n",
    "The surface is described using a grid like the following:\n",
    "\n",
    "| | |\n",
    "|:---|:---|\n",
    "|SFFF| (S: starting point, safe)|\n",
    "|FHFH| (F: frozen surface, safe)|\n",
    "|FFFH| (H: hole, fall to your doom)|\n",
    "|HFFG| (G: goal, where the frisbee is located)|  \n",
    "\n",
    "\n",
    "The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the simple version of this problem below. In the following environment, the ice is not slippery, so you can always move in the direction you intend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "env.reset()\n",
    "display(env.render())\n",
    "max_steps = 100\n",
    "for step in range(max_steps):\n",
    "    # take a random action\n",
    "    action = env.action_space.sample()\n",
    "    clear_output(wait=True)\n",
    "    # take the action and observe the outcome state and reward\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    display(env.render())\n",
    "    time.sleep(1)\n",
    "    state = new_state\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the expected reward for such a random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In episode 14, we takes 16 steps to succeed!\n",
      "In episode 55, we takes 18 steps to succeed!\n",
      "In episode 74, we takes 10 steps to succeed!\n",
      "In episode 253, we takes 15 steps to succeed!\n",
      "In episode 270, we takes 19 steps to succeed!\n",
      "In episode 408, we takes 11 steps to succeed!\n",
      "In episode 502, we takes 9 steps to succeed!\n",
      "In episode 581, we takes 10 steps to succeed!\n",
      "In episode 755, we takes 11 steps to succeed!\n",
      "In episode 764, we takes 8 steps to succeed!\n",
      "In episode 783, we takes 20 steps to succeed!\n",
      "In episode 912, we takes 12 steps to succeed!\n",
      "In episode 959, we takes 20 steps to succeed!\n",
      "The expected reward of each episode is 0.013, and the average steps to succeed is 13.76923076923077\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0',is_slippery=False)\n",
    "total_episode = 1000\n",
    "total_reward = 0\n",
    "total_steps = 0\n",
    "max_steps = 100\n",
    "\n",
    "for episode in range(total_episode):\n",
    "    env.reset()\n",
    "    for step in range(max_steps):\n",
    "        # take a random action\n",
    "        action = env.action_space.sample()\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        state = new_state\n",
    "        if done:\n",
    "            if reward == 1.0:\n",
    "                print('In episode {}, we takes {} steps to succeed!'.format(episode, step + 1))\n",
    "                total_reward += reward\n",
    "                total_steps += step + 1\n",
    "            break\n",
    "env.close()\n",
    "print('The expected reward of each episode is {}, and the average steps to succeed is {}'\n",
    "      .format(total_reward / total_episode, total_steps / total_reward\n",
    "             ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, even for this simple version, the random policy obviously works terribly.\n",
    "\n",
    "Now let's try to crack this environment with reinforcement learning!\n",
    "\n",
    "Let's implement Q-learning algorithm with Monte Carlo methods!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's breifly review the bellman equation for Q-learning algorithm.\n",
    "The derivation is actually simple.\n",
    "\n",
    "Considering a specific action $a$ at each state, we have:\n",
    "\n",
    "$Q(s_t, a_t)=R_{t+1}+\\gamma Q(s_{t+1}, a_{t+1})$\n",
    "\n",
    "Also, in state $s_{t+1}$, we would like to choose the action with the max expected reward, that is:\n",
    "\n",
    "$a_{t+1}=argmax_a Q(s_{t+1}, a)$\n",
    "\n",
    "As a result:\n",
    "\n",
    "$Q(s_{t+1}, a_{t+1}) = max_aQ(s_{t+1}, a)$\n",
    "\n",
    "Thus, we get the updated value:\n",
    "\n",
    "$Q(s_t, a_t)=R_{t+1}+\\gamma max_aQ(s_{t+1}, a)$\n",
    "\n",
    "Recall that in Monte Carlo method, we only update the q-value with a small $\\alpha$, i.e.:\n",
    "\n",
    "$Q(s_t, a_t)=Q(s_t, a_t)+\\alpha\\delta_Q=Q(s_t, a_t)+\\alpha(R_{t+1}+\\gamma max_a(Q(s_{t+1}, a)) - Q(s_t, a_t))$\n",
    "\n",
    "which is exactly the updated equation we will use for Q-learning.\n",
    "\n",
    "Please first complete the *bellman_equation_update* function in *qTable.py* bellow to this formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement Q-learning for this problem. This is basically updating the *qtable* with the *bellman_equation_update*.\n",
    "\n",
    "### TODO: Please implement the QTable class in *qTable.py*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward is 0.0, average step is 100.0\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from qTable import QTable\n",
    "\n",
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "total_episode = 10000\n",
    "gamma = 0.95 # discounting rate\n",
    "alpha = 0.8 # learning rate for Q-learning\n",
    "max_steps = 100\n",
    "\n",
    "qTable = QTable(state_size, action_size, alpha, gamma)\n",
    "qTable.train(env, total_episode, max_steps)\n",
    "print(qTable.qtable)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there is no episode that successfully reaches the goal position! Why? Try to think about it yourself before looking at the answers in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two reasons. The first (and not so important) reason is that qTable.qtable is initialized to be all zeros. As a result, the agent would always take one action and cannot update its qtable as it can never reach the goal grid. To deal with the problem, you can initialize the qTable.qtable with random numbers. If you are lucky enough, you may get some successful episodes. But try to rerun the cell for multiple times, you will find that the agent still cannot reach the goal grid for most episode.\n",
    "\n",
    "The second (and important) reason is that at the beginning of the training process, the *qtable* is extremely noisy. If we rely on it to make decision for each step, then it is very likely that we cannot reach the goal position. Moreover, as we cannot get the reward, the *bellman_equation* cannot effectively update the *qtable*, and we may end up with no succesful episode.\n",
    "\n",
    "This is exactly the reason we employ $\\epsilon$-greedy strategy for the training process. Recall that $\\epsilon$-greedy strategy means that the agent, in every state, takes random actions with a $\\epsilon$ probability while takes the optimal action with a $1-\\epsilon$ probability. Actually, this is often called **exploration** in reinforcement learning. At the beginning of the training process, the agent should explore the environment (maybe aggressively) to discover actions that lead to high rewards: this is a key component of reinforcement learning. In this case, the $\\epsilon$ should be large. After some training iterations, the agent should be more clever to take good actions, and thus we can take a smaller $\\epsilon$ to decrease the exploration. This is often called **exploitation** in reinforcement learning, as we try to exploit the existing policy/qtable. The exploration and exploitation trade-off is often a key trade-off in reinforcement learning. Briefly speaking, in the beginning of training, the agent should focus more on the exploration to train a good policy; as training goes by, the agent should pay more and more attention on exploitability. In practice, we often use a exponential decay strategy to decay the exploration probability.\n",
    "\n",
    "### TODO: Add the epsilon-greedy strategy in the *QTable* class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward is 0.012, average step is 7.5015\n",
      "[[0.94148015 0.95099005 0.95099005 0.94148015]\n",
      " [0.94148015 0.         0.96059601 0.95099005]\n",
      " [0.95099005 0.970299   0.95099005 0.96059601]\n",
      " [0.96059601 0.         0.95099005 0.95099005]\n",
      " [0.95099005 0.96059601 0.         0.94148015]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.9801     0.         0.96059601]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.96059601 0.         0.970299   0.95099005]\n",
      " [0.96059601 0.9801     0.9801     0.        ]\n",
      " [0.970299   0.99       0.         0.970299  ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.9801     0.99       0.970299  ]\n",
      " [0.9801     0.99       1.         0.9801    ]\n",
      " [0.         0.         0.         0.        ]]\n",
      "0.9851089768814493\n",
      "In test, Average reward is 1.0, average step is 6.0\n"
     ]
    }
   ],
   "source": [
    "from qTable import QTable\n",
    "\n",
    "env = gym.make('FrozenLake-v0',is_slippery=False)\n",
    "\n",
    "total_episode = 2000\n",
    "\n",
    "gamma = 0.99                 # discounting rate\n",
    "alpha = 0.85                   # learning rate for Q-learning\n",
    "max_steps = 100\n",
    "\n",
    "# exploration parameters\n",
    "epsilon = 1.0                 # the initial exploration probability\n",
    "min_epsilon = 0.1            # the minimum exploration probability \n",
    "epsilon_decay = 0.999999           # the exponential decay rate for exploration prob\n",
    "\n",
    "# Your code here: tune the epsilon, min_epsilon, epsilon_decay yourself to make the testing average reward of your\n",
    "# agent be higher than 0.8.\n",
    "# If may be helpful for you to print out the reward and steps for each episode\n",
    "# to understand whether you require more exploration or exploitation.\n",
    "# Basically, the higher the epsilon_decay, the more exploration you have in the beginning\n",
    "# of training.\n",
    "# It is very imporant for you to practice itself to get a sense of the exploration\n",
    "# and exploitation trade-off.\n",
    "# begin answer\n",
    "\n",
    "# end answer\n",
    "\n",
    "qTable = QTable(state_size, action_size, alpha, gamma,\n",
    "               init_epsilon=epsilon,\n",
    "               min_epsilon=min_epsilon,\n",
    "               epsilon_decay=epsilon_decay)\n",
    "qTable.train(env, total_episode, max_steps)\n",
    "print(qTable.qtable)\n",
    "print(qTable.epsilon)\n",
    "qTable.eval(env,100,100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we can now use the *qtable* for our MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In test, Average reward is 1.0, average step is 6.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0',is_slippery=False)\n",
    "\n",
    "total_episode = 1000\n",
    "\n",
    "qTable.eval(env, total_episode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also interesting to have a look at how the process works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0',is_slippery=False)\n",
    "state = env.reset()\n",
    "display(env.render())\n",
    "max_steps = 100\n",
    "# the table has been trained well; thus we don't need exploration\n",
    "qTable.set_epsilon(0)\n",
    "for step in range(max_steps):\n",
    "    # take a random action\n",
    "    action = qTable.take_action(state)\n",
    "    clear_output(wait=True)\n",
    "    # take the action and observe the outcome state and reward\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    display(env.render())\n",
    "    time.sleep(1)\n",
    "    state = new_state\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, in the determinstic environment, i.e. there is no slippery, the Q-learning learns the optimal strategy which can takes the minimal steps to the goal, which is desirable. Let's see how this works for slippery environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward is 0.131, average step is 15.8737\n",
      "[[9.96320705e-02 1.07975553e-02 1.05778997e-02 9.63201052e-03]\n",
      " [2.56405758e-05 3.63824590e-04 3.10564779e-03 3.82468827e-02]\n",
      " [1.94818562e-04 7.71253828e-03 1.12545106e-03 6.62454791e-04]\n",
      " [1.13410933e-03 1.74014181e-03 5.52083028e-04 5.30214599e-03]\n",
      " [1.50986847e-01 1.16480871e-03 7.35072470e-03 3.92135187e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.96034590e-05 4.70508639e-06 3.91515555e-02 3.54716722e-06]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.21724819e-02 6.03137369e-03 4.35693014e-03 1.37195705e-01]\n",
      " [2.47452448e-04 5.30513005e-01 9.42570580e-03 5.35415330e-03]\n",
      " [8.35468186e-01 1.20577416e-04 5.55170284e-04 2.41254868e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [7.52787405e-03 4.21314530e-04 5.16778322e-01 3.65594679e-04]\n",
      " [1.14999288e-01 9.90591195e-01 1.34619341e-01 1.45585718e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
      "0.01\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "total_episode = 60000\n",
    "\n",
    "gamma = 0.911 # discounting rate\n",
    "alpha = 0.769 # learning rate for Q-learning\n",
    "max_steps = 100\n",
    "\n",
    "# exploration parameters\n",
    "epsilon = 1.0                 # the initial exploration probability\n",
    "min_epsilon = 0.01             # the minimum exploration probability \n",
    "epsilon_decay = 0.999995           # the exponential decay rate for exploration prob\n",
    "# Your code here: tune the epsilon, min_epsilon, epsilon_decay yourself to make the testing average reward of your\n",
    "# agent be higher than 0.7.\n",
    "# If may be helpful for you to print out the reward and steps for each episode\n",
    "# to understand whether you require more exploration or exploitation.\n",
    "# Basically, the higher the epsilon_decay, the more exploration you have in the beginning\n",
    "# of training.\n",
    "# It is very imporant for you to practice itself to get a sense of the exploration\n",
    "# and exploitation trade-off.\n",
    "\n",
    "# begin answer\n",
    "# end answer\n",
    "\n",
    "qTable = QTable(state_size, action_size, alpha, gamma,\n",
    "               init_epsilon=epsilon,\n",
    "               min_epsilon=min_epsilon,\n",
    "               epsilon_decay=epsilon_decay)\n",
    "qTable.train(env, total_episode, max_steps)\n",
    "print(qTable.qtable)\n",
    "print(qTable.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "scrolled": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In test, Average reward is 0.727, average step is 43.478\n"
     ]
    }
   ],
   "source": [
    "total_episode = 1000\n",
    "\n",
    "qTable.eval(env, total_episode)\n",
    "#print(qTable.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, the expected reward for this non-deterministic environment is lower than the reward for the previous one. However, Q-learning still works pretty good for this setting. Note that we may never get a perfect expected reward for this problem due to the random slippery. Also have a look at the demo below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "state = env.reset()\n",
    "display(env.render())\n",
    "max_steps = 100\n",
    "qTable.set_epsilon(0.0)\n",
    "for step in range(max_steps):\n",
    "    # take a random action\n",
    "    action = qTable.take_action(state)\n",
    "    clear_output(wait=True)\n",
    "    # take the action and observe the outcome state and reward\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    display(env.render())\n",
    "    time.sleep(1)\n",
    "    state = new_state\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's crack a little bit harder game. This environment is discrete but infinite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Chain\n",
    "\n",
    "This game presents moves along a linear chain of states, with two actions:\n",
    "* 0. forward, which moves along the chain but returns no reward\n",
    "* 1. backward, which returns to the beginning and has a small reward\n",
    "\n",
    "The end of the chain, however, presents a large reward, and by moving 'forward' at the end of the chain this large reward can be repeated.\n",
    "\n",
    "At each action, there is a small probability that the agent 'slips' and the opposite transition is instead taken.\n",
    "\n",
    "The observed state is the current state in the chain (0 to n-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward is 129.084\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('NChain-v0',\n",
    "               n=5, # the length of the chain is 5\n",
    "               slip=0.2, # the slipping probability\n",
    "               small=2, # the small reward for backward\n",
    "               large=10 # the large reward for go forward\n",
    "              )\n",
    "state = env.reset()\n",
    "total_episode = 1000\n",
    "max_steps = 100\n",
    "total_reward = 0\n",
    "for episode in range(total_episode):\n",
    "    state = env.reset()\n",
    "    for step in range(max_steps):\n",
    "        # take a random action\n",
    "        action = env.action_space.sample()\n",
    "        # take the action and observe the outcome state and reward\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        #print(new_state,state,action,reward)\n",
    "        total_reward += reward\n",
    "        state = new_state\n",
    "env.close()\n",
    "print('Average reward is {}'.format(total_reward / total_episode))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "It's actually interesting to try strategy of always going forward or backward. Have a try here, and explain such phenomena in your PDF writeup file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply Q-learning for this environment and see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward is 253.86132, average step is 100.0\n",
      "[[37.10409806 28.40088848]\n",
      " [39.2325401  29.49093888]\n",
      " [39.42308399 34.62648736]\n",
      " [52.93964129 32.64171856]\n",
      " [50.82287643 35.44980342]]\n",
      "0.01\n",
      "In test, Average reward is 357.054, average step is 100.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('NChain-v0',\n",
    "               n=5, # the length of the chain is 5\n",
    "               slip=0.2, # the slipping probability\n",
    "               small=2, # the small reward for backward\n",
    "               large=10 # the large reward for go forward\n",
    "              )\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "\n",
    "gamma = 0.94 # discounting rate\n",
    "alpha = 0.4 # learning rate for Q-learning\n",
    "\n",
    "# exploration parameters\n",
    "epsilon = 1.0                 # the initial exploration probability\n",
    "min_epsilon = 0.01            # the minimum exploration probability \n",
    "epsilon_decay = 0.999999      # the exponential decay rate for exploration prob\n",
    "\n",
    "qTable = QTable(state_size, action_size, alpha, gamma,\n",
    "               init_epsilon=epsilon,\n",
    "               min_epsilon=min_epsilon,\n",
    "               epsilon_decay=epsilon_decay)\n",
    "total_episode = 50000\n",
    "max_steps = 100\n",
    "qTable.train(env, total_episode, max_steps)\n",
    "print(qTable.qtable)\n",
    "print(qTable.epsilon)\n",
    "qTable.eval(env, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 denotes going forward, and 1 denotes going backward. Please pay attention to how the reward for going forward and backward change with the state. Also, try to tune the gamma parameter to see how this may influence the expected reward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the continous and infinite environment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) CartPole\n",
    "\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "\n",
    "Have a look at this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "\n",
    "for i in range(100):\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    display(plt.gcf())    \n",
    "    clear_output(wait=True)\n",
    "    new_state, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, have a sense of how a random approach works for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00669996 -0.16516914  0.01623544  0.3025486 ] [ 0.02105403 -0.03670092  0.02861291  0.01416303] 0 1.0 False\n",
      "[ 0.00339658 -0.36051868  0.02228641  0.60030729] [ 0.00669996 -0.16516914  0.01623544  0.3025486 ] 0 1.0 False\n",
      "[-0.00381379 -0.16571548  0.03429255  0.31472672] [ 0.00339658 -0.36051868  0.02228641  0.60030729] 1 1.0 False\n",
      "[-0.0071281  -0.36130872  0.04058709  0.61802411] [-0.00381379 -0.16571548  0.03429255  0.31472672] 0 1.0 False\n",
      "[-0.01435428 -0.55697342  0.05294757  0.92320878] [-0.0071281  -0.36130872  0.04058709  0.61802411] 0 1.0 False\n",
      "[-0.02549375 -0.75276916  0.07141175  1.23205014] [-0.01435428 -0.55697342  0.05294757  0.92320878] 0 1.0 False\n",
      "[-0.04054913 -0.55863453  0.09605275  0.9625676 ] [-0.02549375 -0.75276916  0.07141175  1.23205014] 1 1.0 False\n",
      "[-0.05172182 -0.75490678  0.1153041   1.28381499] [-0.04054913 -0.55863453  0.09605275  0.9625676 ] 0 1.0 False\n",
      "[-0.06681996 -0.95129254  0.1409804   1.61026194] [-0.05172182 -0.75490678  0.1153041   1.28381499] 0 1.0 False\n",
      "[-0.08584581 -1.14777037  0.17318564  1.94336578] [-0.06681996 -0.95129254  0.1409804   1.61026194] 0 1.0 False\n",
      "[-0.10880121 -1.34426482  0.21205296  2.2843608 ] [-0.08584581 -1.14777037  0.17318564  1.94336578] 0 1.0 True\n",
      "[-0.00437369  0.21367172 -0.04063593 -0.30623876] [-0.10880121 -1.34426482  0.21205296  2.2843608 ] 1 1.0 False\n",
      "[-0.00010026  0.01915168 -0.0467607  -0.02664325] [-0.00437369  0.21367172 -0.04063593 -0.30623876] 0 1.0 False\n",
      "[ 0.00028278 -0.17526958 -0.04729357  0.25092723] [-0.00010026  0.01915168 -0.0467607  -0.02664325] 0 1.0 False\n",
      "[-0.00322262  0.0204947  -0.04227502 -0.05628998] [ 0.00028278 -0.17526958 -0.04729357  0.25092723] 1 1.0 False\n",
      "[-0.00281272 -0.1739964  -0.04340082  0.22276086] [-0.00322262  0.0204947  -0.04227502 -0.05628998] 0 1.0 False\n",
      "[-0.00629265  0.02171814 -0.0389456  -0.08329034] [-0.00281272 -0.1739964  -0.04340082  0.22276086] 1 1.0 False\n",
      "[-0.00585829 -0.17282452 -0.04061141  0.19685526] [-0.00629265  0.02171814 -0.0389456  -0.08329034] 0 1.0 False\n",
      "[-0.00931478 -0.36734276 -0.03667431  0.47645556] [-0.00585829 -0.17282452 -0.04061141  0.19685526] 0 1.0 False\n",
      "[-0.01666163 -0.17172268 -0.0271452   0.17244293] [-0.00931478 -0.36734276 -0.03667431  0.47645556] 1 1.0 False\n",
      "[-0.02009609  0.02377705 -0.02369634 -0.12867835] [-0.01666163 -0.17172268 -0.0271452   0.17244293] 1 1.0 False\n",
      "[-0.01962055  0.21923031 -0.0262699  -0.42874199] [-0.02009609  0.02377705 -0.02369634 -0.12867835] 1 1.0 False\n",
      "[-0.01523594  0.02449005 -0.03484474 -0.14445486] [-0.01962055  0.21923031 -0.0262699  -0.42874199] 0 1.0 False\n",
      "[-0.01474614  0.22009324 -0.03773384 -0.44792383] [-0.01523594  0.02449005 -0.03484474 -0.14445486] 1 1.0 False\n",
      "[-0.01034427  0.41572808 -0.04669232 -0.75225831] [-0.01474614  0.22009324 -0.03773384 -0.44792383] 1 1.0 False\n",
      "[-0.00202971  0.61146173 -0.06173748 -1.05926134] [-0.01034427  0.41572808 -0.04669232 -0.75225831] 1 1.0 False\n",
      "[ 0.01019952  0.4172095  -0.08292271 -0.78657741] [-0.00202971  0.61146173 -0.06173748 -1.05926134] 0 1.0 False\n",
      "[ 0.01854371  0.61336678 -0.09865426 -1.10415365] [ 0.01019952  0.4172095  -0.08292271 -0.78657741] 1 1.0 False\n",
      "[ 0.03081105  0.41967085 -0.12073733 -0.84397982] [ 0.01854371  0.61336678 -0.09865426 -1.10415365] 0 1.0 False\n",
      "[ 0.03920447  0.61621525 -0.13761693 -1.17206078] [ 0.03081105  0.41967085 -0.12073733 -0.84397982] 1 1.0 False\n",
      "[ 0.05152877  0.42312413 -0.16105814 -0.9254942 ] [ 0.03920447  0.61621525 -0.13761693 -1.17206078] 0 1.0 False\n",
      "[ 0.05999125  0.2305007  -0.17956803 -0.68744506] [ 0.05152877  0.42312413 -0.16105814 -0.9254942 ] 0 1.0 False\n",
      "[ 0.06460127  0.03826537 -0.19331693 -0.45623825] [ 0.05999125  0.2305007  -0.17956803 -0.68744506] 0 1.0 False\n",
      "[ 0.06536657 -0.15367336 -0.20244169 -0.23017503] [ 0.06460127  0.03826537 -0.19331693 -0.45623825] 0 1.0 False\n",
      "[ 0.06229311 -0.3454139  -0.20704519 -0.00754979] [ 0.06536657 -0.15367336 -0.20244169 -0.23017503] 0 1.0 False\n",
      "[ 0.05538483 -0.14801713 -0.20719619 -0.35775844] [ 0.06229311 -0.3454139  -0.20704519 -0.00754979] 1 1.0 False\n",
      "[ 0.05242449 -0.33968409 -0.21435136 -0.13688795] [ 0.05538483 -0.14801713 -0.20719619 -0.35775844] 0 1.0 True\n",
      "[-1.42861569e-04  2.27078651e-01  3.98757670e-02 -3.08280601e-01] [ 0.05242449 -0.33968409 -0.21435136 -0.13688795] 1 1.0 False\n",
      "[ 0.00439871  0.42161041  0.03371015 -0.58812591] [-1.42861569e-04  2.27078651e-01  3.98757670e-02 -3.08280601e-01] 1 1.0 False\n",
      "[ 0.01283092  0.226033    0.02194764 -0.28501756] [ 0.00439871  0.42161041  0.03371015 -0.58812591] 0 1.0 False\n",
      "[0.01735158 0.03060501 0.01624729 0.01450591] [ 0.01283092  0.226033    0.02194764 -0.28501756] 0 1.0 False\n",
      "[ 0.01796368  0.22549023  0.0165374  -0.27300685] [0.01735158 0.03060501 0.01624729 0.01450591] 1 1.0 False\n",
      "[0.02247348 0.03013627 0.01107727 0.02484581] [ 0.01796368  0.22549023  0.0165374  -0.27300685] 0 1.0 False\n",
      "[ 0.02307621 -0.16514278  0.01157418  0.32100305] [0.02247348 0.03013627 0.01107727 0.02484581] 0 1.0 False\n",
      "[ 0.01977335 -0.36042762  0.01799424  0.61731344] [ 0.02307621 -0.16514278  0.01157418  0.32100305] 0 1.0 False\n",
      "[ 0.0125648  -0.55579627  0.03034051  0.91560898] [ 0.01977335 -0.36042762  0.01799424  0.61731344] 0 1.0 False\n",
      "[ 0.00144888 -0.36109747  0.04865269  0.63261395] [ 0.0125648  -0.55579627  0.03034051  0.91560898] 1 1.0 False\n",
      "[-0.00577307 -0.55686321  0.06130497  0.94021332] [ 0.00144888 -0.36109747  0.04865269  0.63261395] 0 1.0 False\n",
      "[-0.01691034 -0.36261877  0.08010924  0.66740638] [-0.00577307 -0.55686321  0.06130497  0.94021332] 1 1.0 False\n",
      "[-0.02416271 -0.55875801  0.09345737  0.98419865] [-0.01691034 -0.36261877  0.08010924  0.66740638] 0 1.0 False\n",
      "[-0.03533787 -0.36500386  0.11314134  0.72227221] [-0.02416271 -0.55875801  0.09345737  0.98419865] 1 1.0 False\n",
      "[-0.04263795 -0.5614939   0.12758678  1.04831545] [-0.03533787 -0.36500386  0.11314134  0.72227221] 0 1.0 False\n",
      "[-0.05386783 -0.75805655  0.14855309  1.37817173] [-0.04263795 -0.5614939   0.12758678  1.04831545] 0 1.0 False\n",
      "[-0.06902896 -0.56506894  0.17611653  1.13539274] [-0.05386783 -0.75805655  0.14855309  1.37817173] 1 1.0 False\n",
      "[-0.08033034 -0.76200206  0.19882438  1.47773403] [-0.06902896 -0.56506894  0.17611653  1.13539274] 0 1.0 False\n",
      "[-0.09557038 -0.95891797  0.22837906  1.82535888] [-0.08033034 -0.76200206  0.19882438  1.47773403] 0 1.0 True\n",
      "[ 0.03338609  0.23361663 -0.01404924 -0.30817437] [-0.09557038 -0.95891797  0.22837906  1.82535888] 1 1.0 False\n",
      "[ 0.03805843  0.03869765 -0.02021273 -0.01995509] [ 0.03338609  0.23361663 -0.01404924 -0.30817437] 0 1.0 False\n",
      "[ 0.03883238  0.23410356 -0.02061183 -0.31894621] [ 0.03805843  0.03869765 -0.02021273 -0.01995509] 1 1.0 False\n",
      "[ 0.04351445  0.42951291 -0.02699076 -0.61805743] [ 0.03883238  0.23410356 -0.02061183 -0.31894621] 1 1.0 False\n",
      "[ 0.05210471  0.23477818 -0.0393519  -0.33399604] [ 0.04351445  0.42951291 -0.02699076 -0.61805743] 0 1.0 False\n",
      "[ 0.05680027  0.04023776 -0.04603182 -0.0539778 ] [ 0.05210471  0.23477818 -0.0393519  -0.33399604] 0 1.0 False\n",
      "[ 0.05760503  0.23598848 -0.04711138 -0.36082143] [ 0.05680027  0.04023776 -0.04603182 -0.0539778 ] 1 1.0 False\n",
      "[ 0.0623248   0.04156675 -0.05432781 -0.08335803] [ 0.05760503  0.23598848 -0.04711138 -0.36082143] 0 1.0 False\n",
      "[ 0.06315613 -0.15273604 -0.05599497  0.19170162] [ 0.0623248   0.04156675 -0.05432781 -0.08335803] 0 1.0 False\n",
      "[ 0.06010141 -0.3470141  -0.05216094  0.46620805] [ 0.06315613 -0.15273604 -0.05599497  0.19170162] 0 1.0 False\n",
      "[ 0.05316113 -0.15119545 -0.04283678  0.1575512 ] [ 0.06010141 -0.3470141  -0.05216094  0.46620805] 1 1.0 False\n",
      "[ 0.05013722  0.04451279 -0.03968575 -0.14833203] [ 0.05316113 -0.15119545 -0.04283678  0.1575512 ] 1 1.0 False\n",
      "[ 0.05102748 -0.15001905 -0.04265239  0.13157143] [ 0.05013722  0.04451279 -0.03968575 -0.14833203] 0 1.0 False\n",
      "[ 0.0480271   0.0456871  -0.04002096 -0.17425681] [ 0.05102748 -0.15001905 -0.04265239  0.13157143] 1 1.0 False\n",
      "[ 0.04894084 -0.1488399  -0.0435061   0.10553703] [ 0.0480271   0.0456871  -0.04002096 -0.17425681] 0 1.0 False\n",
      "[ 0.04596404  0.04687764 -0.04139536 -0.20054825] [ 0.04894084 -0.1488399  -0.0435061   0.10553703] 1 1.0 False\n",
      "[ 0.04690159 -0.14762857 -0.04540633  0.07879436] [ 0.04596404  0.04687764 -0.04139536 -0.20054825] 0 1.0 False\n",
      "[ 0.04394902  0.0481139  -0.04383044 -0.22786159] [ 0.04690159 -0.14762857 -0.04540633  0.07879436] 1 1.0 False\n",
      "[ 0.0449113  -0.14635516 -0.04838767  0.05067983] [ 0.04394902  0.0481139  -0.04383044 -0.22786159] 0 1.0 False\n",
      "[ 0.0419842  -0.34075109 -0.04737407  0.327712  ] [ 0.0449113  -0.14635516 -0.04838767  0.05067983] 0 1.0 False\n",
      "[ 0.03516917 -0.5351677  -0.04081983  0.60508697] [ 0.0419842  -0.34075109 -0.04737407  0.327712  ] 0 1.0 False\n",
      "[ 0.02446582 -0.72969576 -0.02871809  0.88463829] [ 0.03516917 -0.5351677  -0.04081983  0.60508697] 0 1.0 False\n",
      "[ 0.0098719  -0.92441626 -0.01102533  1.16815664] [ 0.02446582 -0.72969576 -0.02871809  0.88463829] 0 1.0 False\n",
      "[-0.00861642 -1.11939304  0.0123378   1.45736266] [ 0.0098719  -0.92441626 -0.01102533  1.16815664] 0 1.0 False\n",
      "[-0.03100428 -0.92442462  0.04148506  1.16855951] [-0.00861642 -1.11939304  0.0123378   1.45736266] 1 1.0 False\n",
      "[-0.04949277 -0.72986618  0.06485625  0.88916605] [-0.03100428 -0.92442462  0.04148506  1.16855951] 1 1.0 False\n",
      "[-0.0640901  -0.92580545  0.08263957  1.2015114 ] [-0.04949277 -0.72986618  0.06485625  0.88916605] 0 1.0 False\n",
      "[-0.08260621 -1.12189316  0.1066698   1.51890757] [-0.0640901  -0.92580545  0.08263957  1.2015114 ] 0 1.0 False\n",
      "[-0.10504407 -1.31813093  0.13704795  1.84289264] [-0.08260621 -1.12189316  0.1066698   1.51890757] 0 1.0 False\n",
      "[-0.13140669 -1.51447301  0.1739058   2.17481037] [-0.10504407 -1.31813093  0.13704795  1.84289264] 0 1.0 False\n",
      "[-0.16169615 -1.32141962  0.21740201  1.94046917] [-0.13140669 -1.51447301  0.1739058   2.17481037] 1 1.0 True\n",
      "[ 0.0471908   0.22602452  0.04467352 -0.31017609] [-0.16169615 -1.32141962  0.21740201  1.94046917] 1 1.0 False\n",
      "[ 0.05171129  0.42048245  0.03847    -0.58844233] [ 0.0471908   0.22602452  0.04467352 -0.31017609] 1 1.0 False\n",
      "[ 0.06012094  0.61504516  0.02670115 -0.86876308] [ 0.05171129  0.42048245  0.03847    -0.58844233] 1 1.0 False\n",
      "[ 0.07242184  0.80979386  0.00932589 -1.15293279] [ 0.06012094  0.61504516  0.02670115 -0.86876308] 1 1.0 False\n",
      "[ 0.08861772  1.00479293 -0.01373277 -1.4426769 ] [ 0.07242184  0.80979386  0.00932589 -1.15293279] 1 1.0 False\n",
      "[ 0.10871358  0.80984271 -0.04258631 -1.15431645] [ 0.08861772  1.00479293 -0.01373277 -1.4426769 ] 0 1.0 False\n",
      "[ 0.12491043  0.61530123 -0.06567263 -0.87528539] [ 0.10871358  0.80984271 -0.04258631 -1.15431645] 0 1.0 False\n",
      "[ 0.13721646  0.81125151 -0.08317834 -1.18787109] [ 0.12491043  0.61530123 -0.06567263 -0.87528539] 1 1.0 False\n",
      "[ 0.15344149  0.61730055 -0.10693576 -0.92237672] [ 0.13721646  0.81125151 -0.08317834 -1.18787109] 0 1.0 False\n",
      "[ 0.1657875   0.42377327 -0.1253833  -0.66512323] [ 0.15344149  0.61730055 -0.10693576 -0.92237672] 0 1.0 False\n",
      "[ 0.17426296  0.62039565 -0.13868576 -0.99450769] [ 0.1657875   0.42377327 -0.1253833  -0.66512323] 1 1.0 False\n",
      "[ 0.18667088  0.81707274 -0.15857592 -1.32733377] [ 0.17426296  0.62039565 -0.13868576 -0.99450769] 1 1.0 False\n",
      "[ 0.20301233  1.01380059 -0.18512259 -1.66514925] [ 0.18667088  0.81707274 -0.15857592 -1.32733377] 1 1.0 False\n",
      "[ 0.22328834  1.2105316  -0.21842558 -2.00931936] [ 0.20301233  1.01380059 -0.18512259 -1.66514925] 1 1.0 True\n",
      "[-0.00646266 -0.15886097  0.02588479  0.25123764] [ 0.22328834  1.2105316  -0.21842558 -2.00931936] 0 1.0 False\n",
      "[-0.00963988  0.03588197  0.03090954 -0.03316964] [-0.00646266 -0.15886097  0.02588479  0.25123764] 1 1.0 False\n",
      "[-0.00892224  0.23054734  0.03024615 -0.31594227] [-0.00963988  0.03588197  0.03090954 -0.03316964] 1 1.0 False\n",
      "[-0.00431129  0.03500791  0.0239273  -0.01387626] [-0.00892224  0.23054734  0.03024615 -0.31594227] 0 1.0 False\n",
      "[-0.00361113  0.22977869  0.02364978 -0.29891484] [-0.00431129  0.03500791  0.0239273  -0.01387626] 1 1.0 False\n",
      "[ 0.00098444  0.42455569  0.01767148 -0.58404625] [-0.00361113  0.22977869  0.02364978 -0.29891484] 1 1.0 False\n",
      "[ 0.00947556  0.2291907   0.00599056 -0.28584937] [ 0.00098444  0.42455569  0.01767148 -0.58404625] 0 1.0 False\n",
      "[0.01405937 0.03398382 0.00027357 0.00871689] [ 0.00947556  0.2291907   0.00599056 -0.28584937] 0 1.0 False\n",
      "[ 0.01473905  0.22910185  0.00044791 -0.28387971] [0.01405937 0.03398382 0.00027357 0.00871689] 1 1.0 False\n",
      "[ 0.01932108  0.42421741 -0.00522969 -0.57642133] [ 0.01473905  0.22910185  0.00044791 -0.28387971] 1 1.0 False\n",
      "[ 0.02780543  0.61941228 -0.01675811 -0.87074715] [ 0.01932108  0.42421741 -0.00522969 -0.57642133] 1 1.0 False\n",
      "[ 0.04019368  0.42452222 -0.03417306 -0.58337977] [ 0.02780543  0.61941228 -0.01675811 -0.87074715] 0 1.0 False\n",
      "[ 0.04868412  0.62010584 -0.04584065 -0.88662883] [ 0.04019368  0.42452222 -0.03417306 -0.58337977] 1 1.0 False\n",
      "[ 0.06108624  0.81581907 -0.06357323 -1.19336272] [ 0.04868412  0.62010584 -0.04584065 -0.88662883] 1 1.0 False\n",
      "[ 0.07740262  1.01170418 -0.08744048 -1.50527477] [ 0.06108624  0.81581907 -0.06357323 -1.19336272] 1 1.0 False\n",
      "[ 0.0976367   1.20777132 -0.11754598 -1.82392662] [ 0.07740262  1.01170418 -0.08744048 -1.50527477] 1 1.0 False\n",
      "[ 0.12179213  1.40398548 -0.15402451 -2.15069586] [ 0.0976367   1.20777132 -0.11754598 -1.82392662] 1 1.0 False\n",
      "[ 0.14987184  1.21067829 -0.19703843 -1.9092721 ] [ 0.12179213  1.40398548 -0.15402451 -2.15069586] 0 1.0 False\n",
      "[ 0.1740854   1.40730587 -0.23522387 -2.25606174] [ 0.14987184  1.21067829 -0.19703843 -1.9092721 ] 1 1.0 True\n",
      "[ 0.04921271 -0.21412553 -0.03211348  0.31037263] [ 0.1740854   1.40730587 -0.23522387 -2.25606174] 0 1.0 False\n",
      "[ 0.0449302  -0.40877558 -0.02590603  0.59275742] [ 0.04921271 -0.21412553 -0.03211348  0.31037263] 0 1.0 False\n",
      "[ 0.03675469 -0.60352549 -0.01405088  0.87716874] [ 0.0449302  -0.40877558 -0.02590603  0.59275742] 0 1.0 False\n",
      "[ 0.02468418 -0.40821542  0.00349249  0.58010173] [ 0.03675469 -0.60352549 -0.01405088  0.87716874] 1 1.0 False\n",
      "[ 0.01651987 -0.60338614  0.01509453  0.87388282] [ 0.02468418 -0.40821542  0.00349249  0.58010173] 0 1.0 False\n",
      "[ 0.00445215 -0.40847263  0.03257219  0.58598349] [ 0.01651987 -0.60338614  0.01509453  0.87388282] 1 1.0 False\n",
      "[-0.0037173  -0.6040353   0.04429186  0.88874642] [ 0.00445215 -0.40847263  0.03257219  0.58598349] 0 1.0 False\n",
      "[-0.01579801 -0.79972944  0.06206678  1.1950173 ] [-0.0037173  -0.6040353   0.04429186  0.88874642] 0 1.0 False\n",
      "[-0.0317926  -0.99559768  0.08596713  1.50648985] [-0.01579801 -0.79972944  0.06206678  1.1950173 ] 0 1.0 False\n",
      "[-0.05170455 -1.19165059  0.11609693  1.82472643] [-0.0317926  -0.99559768  0.08596713  1.50648985] 0 1.0 False\n",
      "[-0.07553756 -0.99799257  0.15259146  1.57025074] [-0.05170455 -1.19165059  0.11609693  1.82472643] 1 1.0 False\n",
      "[-0.09549741 -1.19457113  0.18399647  1.90638035] [-0.07553756 -0.99799257  0.15259146  1.57025074] 0 1.0 False\n",
      "[-0.11938884 -1.00185212  0.22212408  1.67597164] [-0.09549741 -1.19457113  0.18399647  1.90638035] 1 1.0 True\n",
      "[ 0.00537805  0.2101149   0.02494407 -0.2796373 ] [-0.11938884 -1.00185212  0.22212408  1.67597164] 1 1.0 False\n",
      "[0.00958035 0.01464616 0.01935133 0.02080739] [ 0.00537805  0.2101149   0.02494407 -0.2796373 ] 0 1.0 False\n",
      "[ 0.00987327  0.20948533  0.01976748 -0.2657077 ] [0.00958035 0.01464616 0.01935133 0.02080739] 1 1.0 False\n",
      "[ 0.01406298  0.40431965  0.01445332 -0.55209082] [ 0.00987327  0.20948533  0.01976748 -0.2657077 ] 1 1.0 False\n",
      "[ 0.02214937  0.59923566  0.00341151 -0.84018518] [ 0.01406298  0.40431965  0.01445332 -0.55209082] 1 1.0 False\n",
      "[ 0.03413409  0.4040673  -0.0133922  -0.54643136] [ 0.02214937  0.59923566  0.00341151 -0.84018518] 0 1.0 False\n",
      "[ 0.04221543  0.20913605 -0.02432082 -0.25799789] [ 0.03413409  0.4040673  -0.0133922  -0.54643136] 0 1.0 False\n",
      "[ 0.04639815  0.40459662 -0.02948078 -0.55825166] [ 0.04221543  0.20913605 -0.02432082 -0.25799789] 1 1.0 False\n",
      "[ 0.05449009  0.20990064 -0.04064582 -0.27500068] [ 0.04639815  0.40459662 -0.02948078 -0.55825166] 0 1.0 False\n",
      "[ 0.0586881   0.40557823 -0.04614583 -0.58022123] [ 0.05449009  0.20990064 -0.04064582 -0.27500068] 1 1.0 False\n",
      "[ 0.06679966  0.60131541 -0.05775025 -0.88707651] [ 0.0586881   0.40557823 -0.04614583 -0.58022123] 1 1.0 False\n",
      "[ 0.07882597  0.79717169 -0.07549178 -1.1973403 ] [ 0.06679966  0.60131541 -0.05775025 -0.88707651] 1 1.0 False\n",
      "[ 0.09476941  0.60310358 -0.09943859 -0.92924075] [ 0.07882597  0.79717169 -0.07549178 -1.1973403 ] 0 1.0 False\n",
      "[ 0.10683148  0.79941705 -0.11802341 -1.25144307] [ 0.09476941  0.60310358 -0.09943859 -0.92924075] 1 1.0 False\n",
      "[ 0.12281982  0.60598837 -0.14305227 -0.99793687] [ 0.10683148  0.79941705 -0.11802341 -1.25144307] 0 1.0 False\n",
      "[ 0.13493959  0.41303856 -0.163011   -0.75338257] [ 0.12281982  0.60598837 -0.14305227 -0.99793687] 0 1.0 False\n",
      "[ 0.14320036  0.22049443 -0.17807866 -0.51610845] [ 0.13493959  0.41303856 -0.163011   -0.75338257] 0 1.0 False\n",
      "[ 0.14761025  0.02826805 -0.18840082 -0.28440756] [ 0.14320036  0.22049443 -0.17807866 -0.51610845] 0 1.0 False\n",
      "[ 0.14817561 -0.16373742 -0.19408898 -0.05655841] [ 0.14761025  0.02826805 -0.18840082 -0.28440756] 0 1.0 False\n",
      "[ 0.14490086  0.03356103 -0.19522014 -0.40365389] [ 0.14817561 -0.16373742 -0.19408898 -0.05655841] 1 1.0 False\n",
      "[ 0.14557208 -0.15833425 -0.20329322 -0.1783094 ] [ 0.14490086  0.03356103 -0.19522014 -0.40365389] 0 1.0 False\n",
      "[ 0.14240539  0.03902865 -0.20685941 -0.52761467] [ 0.14557208 -0.15833425 -0.20329322 -0.1783094 ] 1 1.0 False\n",
      "[ 0.14318597 -0.15267461 -0.2174117  -0.3065741 ] [ 0.14240539  0.03902865 -0.20685941 -0.52761467] 0 1.0 True\n",
      "[-0.0039975   0.17772122  0.02458065 -0.27775338] [ 0.14318597 -0.15267461 -0.2174117  -0.3065741 ] 1 1.0 False\n",
      "[-4.43071921e-04  3.72484028e-01  1.90255865e-02 -5.62583363e-01] [-0.0039975   0.17772122  0.02458065 -0.27775338] 1 1.0 False\n",
      "[ 0.00700661  0.5673339   0.00777392 -0.8492121 ] [-4.43071921e-04  3.72484028e-01  1.90255865e-02 -5.62583363e-01] 1 1.0 False\n",
      "[ 0.01835329  0.76234898 -0.00921032 -1.13944036] [ 0.00700661  0.5673339   0.00777392 -0.8492121 ] 1 1.0 False\n",
      "[ 0.03360027  0.56734865 -0.03199913 -0.84966007] [ 0.01835329  0.76234898 -0.00921032 -1.13944036] 0 1.0 False\n",
      "[ 0.04494724  0.37267735 -0.04899233 -0.56720874] [ 0.03360027  0.56734865 -0.03199913 -0.84966007] 0 1.0 False\n",
      "[ 0.05240079  0.56845107 -0.06033651 -0.87491495] [ 0.04494724  0.37267735 -0.04899233 -0.56720874] 1 1.0 False\n",
      "[ 0.06376981  0.76433909 -0.07783481 -1.18594046] [ 0.05240079  0.56845107 -0.06033651 -0.87491495] 1 1.0 False\n",
      "[ 0.07905659  0.57030804 -0.10155361 -0.9186354 ] [ 0.06376981  0.76433909 -0.07783481 -1.18594046] 0 1.0 False\n",
      "[ 0.09046275  0.76664531 -0.11992632 -1.24142944] [ 0.07905659  0.57030804 -0.10155361 -0.9186354 ] 1 1.0 False\n",
      "[ 0.10579566  0.5732495  -0.14475491 -0.98859322] [ 0.09046275  0.76664531 -0.11992632 -1.24142944] 0 1.0 False\n",
      "[ 0.11726065  0.7699815  -0.16452678 -1.32301435] [ 0.10579566  0.5732495  -0.14475491 -0.98859322] 1 1.0 False\n",
      "[ 0.13266028  0.96675472 -0.19098706 -1.66234128] [ 0.11726065  0.7699815  -0.16452678 -1.32301435] 1 1.0 False\n",
      "[ 0.15199537  1.16351901 -0.22423389 -2.00793062] [ 0.13266028  0.96675472 -0.19098706 -1.66234128] 1 1.0 True\n",
      "[ 0.03853362 -0.24416457  0.00490387  0.28882384] [ 0.15199537  1.16351901 -0.22423389 -2.00793062] 0 1.0 False\n",
      "[ 0.03365033 -0.43935611  0.01068034  0.58304936] [ 0.03853362 -0.24416457  0.00490387  0.28882384] 0 1.0 False\n",
      "[ 0.0248632  -0.2443854   0.02234133  0.29374994] [ 0.03365033 -0.43935611  0.01068034  0.58304936] 1 1.0 False\n",
      "[ 0.0199755  -0.43981864  0.02821633  0.59339444] [ 0.0248632  -0.2443854   0.02234133  0.29374994] 0 1.0 False\n",
      "[ 0.01117912 -0.63532396  0.04008422  0.89483019] [ 0.0199755  -0.43981864  0.02821633  0.59339444] 0 1.0 False\n",
      "[-0.00152736 -0.83096587  0.05798082  1.19983893] [ 0.01117912 -0.63532396  0.04008422  0.89483019] 0 1.0 False\n",
      "[-0.01814667 -0.63663994  0.0819776   0.92587668] [-0.00152736 -0.83096587  0.05798082  1.19983893] 1 1.0 False\n",
      "[-0.03087947 -0.44271507  0.10049514  0.66004068] [-0.01814667 -0.63663994  0.0819776   0.92587668] 1 1.0 False\n",
      "[-0.03973377 -0.63908132  0.11369595  0.98259981] [-0.03087947 -0.44271507  0.10049514  0.66004068] 0 1.0 False\n",
      "[-0.0525154  -0.44565123  0.13334794  0.7276826 ] [-0.03973377 -0.63908132  0.11369595  0.98259981] 1 1.0 False\n",
      "[-0.06142842 -0.64233991  0.1479016   1.05918463] [-0.0525154  -0.44565123  0.13334794  0.7276826 ] 0 1.0 False\n",
      "[-0.07427522 -0.44945348  0.16908529  0.81633847] [-0.06142842 -0.64233991  0.1479016   1.05918463] 1 1.0 False\n",
      "[-0.08326429 -0.25699988  0.18541206  0.58124944] [-0.07427522 -0.44945348  0.16908529  0.81633847] 1 1.0 False\n",
      "[-0.08840429 -0.06489344  0.19703705  0.35222809] [-0.08326429 -0.25699988  0.18541206  0.58124944] 1 1.0 False\n",
      "[-0.08970216  0.12696099  0.20408161  0.12756954] [-0.08840429 -0.06489344  0.19703705  0.35222809] 1 1.0 False\n",
      "[-0.08716294 -0.0704108   0.206633    0.47706765] [-0.08970216  0.12696099  0.20408161  0.12756954] 0 1.0 False\n",
      "[-0.08857115 -0.26775915  0.21617435  0.82711168] [-0.08716294 -0.0704108   0.206633    0.47706765] 0 1.0 True\n",
      "The average reward is 18.7\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state=env.reset()\n",
    "total_reward = 0\n",
    "total_episode = 10\n",
    "for episode in range(total_episode):\n",
    "    env.reset()\n",
    "    while True:\n",
    "        action=env.action_space.sample()\n",
    "        new_state, reward, done, info = env.step(action) # take a random action\n",
    "        print(new_state,state,action,reward,done)\n",
    "        total_reward += reward\n",
    "        state=new_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "env.close()\n",
    "print(\"The average reward is {}\".format(total_reward / total_episode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is continous task. For this game there are a huge number of states available. The state has four values: the position and velocity of the cart, and the position and velocity of the pole. These are all real-valued numbers, so ignoring floating point precisions, you practically have infinite states. Though we can somehow discretize the state and apply the *qtable* approach, we'll replace it with a approximator that will approximate the *qtable* lookup function. Thus we can no longer apply the *qtable* for this task. \n",
    "\n",
    "As a result, our Q value, $Q(s, a)$ is calculated by passing in a state to the approximator. The output will be Q values for each available action.\n",
    "\n",
    "We can define our targets for training as $\\hat{Q}(s,a) = r + \\gamma \\max{Q(s', a')}$. Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$, which is exactly a regression problem.\n",
    "\n",
    "For this Cart-Pole game, we have four inputs, one for each value in the state, and two outputs, one for each action. To get $\\hat{Q}$, we'll first choose an action, then simulate the game using that action. This will get us the next state, $s'$, and the reward. With that, we can calculate $\\hat{Q}$ then pass it back into the $Q$ approximator to run the optimizer and update the weights.\n",
    "\n",
    "Note that since this is a regression task, we can use any regressor we have learned before. At first, we will try to use linear regression for this task. Feel free to use TensorFlow or Pytorch or even the code you have written in previous homework. \n",
    "\n",
    "FYI: GPU should provide more efficient training for these models, though a modern CPU should be enough for this homework. Our laptop (i5) can finish all the training tasks in dozens of minutes.\n",
    "\n",
    "### TODO: implement the Approximator class in *approximator.py* with linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful readings:\n",
    "* https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "* https://www.tensorflow.org/tutorials/reinforcement_learning/actor_critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you like tensorflow\n",
    "# from approximator_tf import Approximator\n",
    "# If you like pytorch (Recommend)\n",
    "from approximator_torch import Approximator\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "gamma = 0.95 # discounting rate\n",
    "\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.shape[0]\n",
    "# tune the learning_rate and total_episode yourself\n",
    "# remember how learning_rate can influence the training for NNs?\n",
    "learning_rate = 0.01\n",
    "total_episode = 10\n",
    "# begin answer\n",
    "# end answer\n",
    "model = Approximator(state_size, action_size,\n",
    "                     learning_rate=learning_rate, memory_pool_size=10000, batch_size=20)\n",
    "\n",
    "model.train(env, total_episode)\n",
    "# plot to see the training rewards of each episode\n",
    "plt.plot(np.arange(len(model.reward_list)), np.array(model.reward_list))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4322, 0.3282, 0.1573],\n",
      "        [0.0476, 0.0625, 0.7571],\n",
      "        [0.0047, 0.5176, 0.4138],\n",
      "        [0.9301, 0.5090, 0.8964],\n",
      "        [0.9773, 0.5739, 0.1606]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how linear approximator struggles to learn a good q-function for this task. The reason might be that the Q function for this problem should be a highly non-linear function; thus a linear approximator cannot approximate it well. We should use a more complex approximator. **In many cases, when you want to use a complex approximator, you should consider about neural networks.**\n",
    "\n",
    "Now we will try to use neural networks as approximator. Feel free to use TensorFlow or Pytorch or even the code you have written in **Assignment3** to implement. Update your Approximator class to use a neural network as the approximator, and rerun the cell above.\n",
    "\n",
    "Use a simple network should be enough for this a task. An example can be:\n",
    "\n",
    "State->FC->Relu->FC->Relu->FC->QValue\n",
    "\n",
    "You can decide the number of neurons yourself, but 64 should be enough.\n",
    "\n",
    "### TODO: implement a neural network in your Approximator class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay\n",
    "\n",
    "Disappointedly, you may find that the approximator still struggle to converge and get good results. There are several potential reasons for this phenomenon. One reason is that the *new_state* is generated from current *state*, which violates the i.i.d assumption in supervised learning. And this may easily lead to bad convergence performance. One popular strategy for improving the convergence rate and making data more i.i.d is **experience replay**.\n",
    "\n",
    "Basically, when using **experience replay**, we would like to keep a memory pool filled with the experience the agent had before. More specifically, we would save the *(state, action, reward, new_state, done)* tuple in a memory pool, which denotes the previous experience. For each step of training, we sample a batch of experiences from the memory pool, and train the approximator with this batch. This can be seen as replaying a batch of previous experiences. Also, we would generate a new experience according to the current state and the approximator, and add this new experience to the memory pool.\n",
    "\n",
    "Note that in this case, the experience an agent has can be used to train the approximator multiple times. In some real-world applications, the experience may be very expensive to gain, and thus experience replay enables a more efficient use of previous experience. Also, as approximator is trained with a batch, the SGD optimizer typically converges more fast.\n",
    "\n",
    "### TODO: implement experience replay in your Approximator class\n",
    "Now please complete the **experience_replay** function in your Approximator class, and retrain the approximator. This time, you should find that the approximator can get pretty reasonable results. Check the average reward for each episode below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reward_list = model.eval(env, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, it may be interesting to have a look at a demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state = env.reset()\n",
    "\n",
    "while True:\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    display(plt.gcf())    \n",
    "    clear_output(wait=True)\n",
    "    action = model.take_action(state)\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    state = new_state\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Double Q-Learning\n",
    "\n",
    "As we only use the same model to both determine the greedy policy and to determine its value, sometimes the model may learn unrealistically high action values because it includes a maximization step over estimated action values, which tends to prefer over-estimated to underestimated values. Imagine that the model learns an over-estimated value for one action, then this action is very more likely to be selected. Further, as the values are estimated with the same model, the updated value for this action is still over-estimated, which makes the other actions difficult to be explored and updated. As a result, the convergence may be slowed down, and even the performance of the final model may be harmed.\n",
    "\n",
    "As can be seen, the over-estimated problem is partly caused by using the same model to determine the policy and value. Then how can we address this problem? A natural idea is to decouple these two steps by using two different models to determine the policy and value respectively. This is called **double Q-learning**. We call the model that determine the policy as Policy Model and the model that determine the value as Value Model. To avoid introduce an additional network, a very smart trick is like this: when training the models with one episode, we freeze the Value Model and update only the Policy Model. After one episode is trained, we update the Value Model with the weights of the Policy Model. Alternatively, we can update the Value Model with the weights of Policy Model periodically (say, 500 iterations). Although the Policy Model and the Value Model are not fully decoupled in this way, this approach is simple to implement and avoid further network designs. As you will see, this works well in practice.\n",
    "\n",
    "### TODO: implement double Q-learning in your Approximator class\n",
    "\n",
    "After implementing the double Q-learning, train another approximator and compare the performance with previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# tune the learning_rate yourself\n",
    "# remember how learning_rate can influence the training for NNs?\n",
    "learning_rate = 0.01\n",
    "# begin answer\n",
    "# end answer\n",
    "model = Approximator(state_size, action_size,\n",
    "                     learning_rate=learning_rate,\n",
    "                     memory_pool_size=10000,\n",
    "                     batch_size=20,\n",
    "                     double_QLearning=True)\n",
    "\n",
    "model.train(env, total_episode)\n",
    "# plot to see the training rewards of each episode\n",
    "plt.plot(np.arange(len(model.reward_list)), np.array(model.reward_list))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the average rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reward_list = model.eval(env, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state = env.reset()\n",
    "\n",
    "while True:\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    display(plt.gcf())    \n",
    "    clear_output(wait=True)\n",
    "    state = np.expand_dims(state, axis=0)\n",
    "    action = model.take_action(state)\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    state = new_state\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have finished the journey of reinforcement learning here. There are also some other environments available in *gym* for you to play with. Equipped with the techniques you have learned so far, just feel free to play with them if interested.\n",
    "\n",
    "Actually, you have learned many practical techniques for reinforcement learning. Theortically, you can even build some famous systems such as AlphaGo mainly based on these techniques :) In AlphaGo, Atari, and many other real-world applications, the agents need to deal with the image/video input. Thus it is naturally to use CNNs as approximators. There are also some other techniques used in these systems, such as Monte Carlo Tree Search in AlphaGo, but the big pictures are similar.\n",
    "\n",
    "To apply reinforcement learning in real-world problems, there are many other difficulties. I think the biggest challenge may be how to formulate your problems. What are the states? What are the rewards? Are they reasonable enough? How to gain experience? Unlike the environment in this notebook, in real world, these problems are often undefined. You need to formulate them yourself. Another challenge may be that the hyperparameters tuning would be quite difficult, even more difficult than CNNs/RNNs for some typical supervised/unsupervised learning problems. It requires some experience and practice :)\n",
    "\n",
    "This may be the last assignment for this course. Thank you for participating our course and completing our assignments :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
